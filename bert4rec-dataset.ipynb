{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":792972,"datasetId":1636,"databundleVersionId":814762}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport csv\nimport glob\nimport zipfile\nimport pandas as pd\nimport numpy as np\nfrom io import BytesIO\nimport requests\nfrom sentence_transformers import SentenceTransformer, util","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T19:12:36.888582Z","iopub.execute_input":"2026-02-17T19:12:36.889371Z","iopub.status.idle":"2026-02-17T19:13:13.550670Z","shell.execute_reply.started":"2026-02-17T19:12:36.889337Z","shell.execute_reply":"2026-02-17T19:13:13.549553Z"}},"outputs":[{"name":"stderr","text":"2026-02-17 19:12:53.557611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771355573.784420      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771355573.849398      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771355574.382653      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771355574.382711      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771355574.382715      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771355574.382717      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class MovieDataProcessor:\n    def __init__(self, movielens_name='ml-32m', target_dir='data/movielens'):\n        self.ml_name = movielens_name\n        self.target_dir = target_dir\n        self.ml_path = os.path.join(target_dir, movielens_name)\n        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n        \n    def download_movielens(self):\n        url = f'http://files.grouplens.org/datasets/movielens/{self.ml_name}.zip'\n        if not os.path.exists(self.target_dir):\n            os.makedirs(self.target_dir)\n        \n        print(f\"Downloading {self.ml_name}...\")\n        r = requests.get(url, stream=True)\n        with zipfile.ZipFile(BytesIO(r.content)) as z:\n            z.extractall(self.target_dir)\n        print(\"MovieLens extracted.\")\n\n    def process_netflix_raw(self, input_pattern, output_csv):\n        \"\"\"Converts Netflix .txt files to a memory-efficient CSV.\"\"\"\n        files = glob.glob(input_pattern)\n        print(f\"Converting {len(files)} Netflix files...\")\n        \n        with open(output_csv, 'w', newline='') as f:\n            writer = csv.writer(f)\n            writer.writerow(['movie_id', 'user_id', 'date'])\n            for file_path in files:\n                with open(file_path, 'r') as infile:\n                    movie_id = None\n                    for line in infile:\n                        line = line.strip()\n                        if line.endswith(':'):\n                            movie_id = line[:-1]\n                        else:\n                            parts = line.split(',')\n                            # parts[2] is YYYY-MM-DD, we strip '-'\n                            writer.writerow([movie_id, parts[0], parts[2].replace('-', '')])\n        \n        # Load a sample/chunk for processing as requested\n        df = pd.read_csv(output_csv, dtype={'movie_id': 'uint16', 'user_id': 'uint32', 'date': 'uint32'}, nrows=5_000_000)\n        df['rating'] = 0 # Default placeholder\n        return df\n\n    def get_unified_id_map(self, netflix_titles, ml_movies):\n        \"\"\"Matches MovieLens IDs to Netflix IDs using Semantic Similarity.\"\"\"\n        print(\"Aligning Movie IDs via Semantic Search...\")\n        \n        # Clean ML titles: \"Toy Story (1995)\" -> (\"Toy Story\", 1995)\n        def clean_ml(t):\n            match = re.search(r'\\((\\d{4})\\)$', str(t))\n            return (t[:match.start()].strip(), int(match.group(1))) if match else (t, None)\n\n        ml_movies[['title_clean', 'year']] = ml_movies['title'].apply(lambda x: pd.Series(clean_ml(x)))\n        \n        # Embeddings\n        net_emb = self.model.encode(netflix_titles['title'].astype(str).tolist(), convert_to_tensor=True)\n        ml_emb = self.model.encode(ml_movies['title_clean'].tolist(), convert_to_tensor=True)\n        \n        hits = util.semantic_search(net_emb, ml_emb, top_k=1)\n        \n        id_map = {}\n        for i, hit in enumerate(hits):\n            best = hit[0]\n            if best['score'] > 0.85:\n                # Check year parity (allow 1 year drift)\n                net_yr = netflix_titles.iloc[i]['year']\n                ml_yr = ml_movies.iloc[best['corpus_id']]['year']\n                if pd.isna(net_yr) or pd.isna(ml_yr) or abs(net_yr - ml_yr) <= 1:\n                    id_map[ml_movies.iloc[best['corpus_id']]['movieId']] = netflix_titles.iloc[i]['movie_id']\n\n        # Handle unmatched ML movies with new IDs\n        max_net_id = netflix_titles['movie_id'].max()\n        unmatched = set(ml_movies['movieId']) - set(id_map.keys())\n        for i, ml_id in enumerate(unmatched):\n            id_map[ml_id] = max_net_id + i + 1\n            \n        return id_map\n\n    def save_user_history(self, df, filename):\n        \"\"\"Filters, sorts, and saves sequence data to Parquet/List.\"\"\"\n        print(f\"Generating sequences for {filename}...\")\n        \n        # Filter users with at least 5 interactions\n        counts = df['user_id'].value_counts()\n        df = df[df['user_id'].isin(counts[counts >= 5].index)].copy()\n        \n        # Sort by user and date\n        df.sort_values(['user_id', 'date'], inplace=True)\n        \n        # Group and take last 100\n        history = df.groupby('user_id')['movie_id'].apply(lambda x: list(x)[-100:]).reset_index(name='list_movies')\n        history.to_parquet(filename.replace('.csv', '.parquet'), index=False)\n        \n        # Create integer-mapped sequences for modeling\n        unique_movies = sorted(list(set([m for seq in history['list_movies'] for m in seq])))\n        movie_to_idx = {m: i + 1 for i, m in enumerate(unique_movies)}\n        \n        return history['list_movies'].apply(lambda seq: [movie_to_idx[m] for m in seq]).tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T19:13:13.552426Z","iopub.execute_input":"2026-02-17T19:13:13.553326Z","iopub.status.idle":"2026-02-17T19:13:13.573469Z","shell.execute_reply.started":"2026-02-17T19:13:13.553290Z","shell.execute_reply":"2026-02-17T19:13:13.572430Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"path = '/kaggle/input/datasets/organizations/netflix-inc/netflix-prize-data'\n!mkdir -p /content/data/netflix_prize\n!cp -r \"{path}\"/* /content/data/netflix_prize/\n!ls /content/data/netflix_prize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T19:18:59.339369Z","iopub.execute_input":"2026-02-17T19:18:59.339704Z","iopub.status.idle":"2026-02-17T19:19:10.643857Z","shell.execute_reply.started":"2026-02-17T19:18:59.339669Z","shell.execute_reply":"2026-02-17T19:19:10.642625Z"}},"outputs":[{"name":"stdout","text":"combined_data_1.txt  combined_data_3.txt  movie_titles.csv  qualifying.txt\ncombined_data_2.txt  combined_data_4.txt  probe.txt\t    README\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"processor = MovieDataProcessor()\n\n# 1. Setup Data\nprocessor.download_movielens()\n# Note: Ensure Kaggle dataset is available at this path\nnetflix_df = processor.process_netflix_raw('/content/data/netflix_prize/combined_data_*.txt', 'temp_netflix.csv')\nnetflix_titles = pd.read_csv('/content/data/netflix_prize/movie_titles.csv', \n                             encoding='ISO-8859-1', names=['movie_id', 'year', 'title'], on_bad_lines='skip')\n\n# 2. Map IDs\nml_movies = pd.read_csv(os.path.join(processor.ml_path, 'movies.csv'))\nid_map = processor.get_unified_id_map(netflix_titles, ml_movies)\n\n# 3. Process MovieLens Ratings\nml_ratings = pd.read_csv(os.path.join(processor.ml_path, 'ratings.csv'))\nml_ratings['movie_id'] = ml_ratings['movieId'].map(id_map)\n# Offset User IDs to avoid collision with Netflix users\nml_ratings['user_id'] = ml_ratings['userId'] + netflix_df['user_id'].max()\nml_ratings['date'] = pd.to_datetime(ml_ratings['timestamp'], unit='s').dt.strftime('%Y%m%d').astype(int)\nml_ratings = ml_ratings[['user_id', 'movie_id', 'date', 'rating']]\n\n# 4. Final Merging\ncombined_df = pd.concat([netflix_df[['user_id', 'movie_id', 'date', 'rating']], ml_ratings], ignore_index=True)\n\n# 5. Output Sequences\nnetflix_sequences = processor.save_user_history(netflix_df, 'user_history_netflix.csv')\nml_sequences = processor.save_user_history(ml_ratings, 'user_history_ml.csv')\ncombined_sequences = processor.save_user_history(combined_df, 'user_history.csv')\n\nprint(\"Pipeline Complete. Sequences generated.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T19:19:14.756670Z","iopub.execute_input":"2026-02-17T19:19:14.757656Z","iopub.status.idle":"2026-02-17T19:31:29.188445Z","shell.execute_reply.started":"2026-02-17T19:19:14.757616Z","shell.execute_reply":"2026-02-17T19:31:29.187298Z"}},"outputs":[{"name":"stdout","text":"Downloading ml-32m...\nMovieLens extracted.\nConverting 4 Netflix files...\nAligning Movie IDs via Semantic Search...\nGenerating sequences for user_history_netflix.csv...\nGenerating sequences for user_history_ml.csv...\nGenerating sequences for user_history.csv...\nPipeline Complete. Sequences generated.\n","output_type":"stream"}],"execution_count":7}]}